{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Fake Video Detection.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"H5cXNX6msvKs"},"source":["## Import libraries"]},{"cell_type":"code","metadata":{"id":"6FLgY0UInyq3"},"source":["import numpy as np # for processing of arrays\n","import pandas as pd\n","import sklearn # to display model performance on test set\n","import statistics\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import classification_report\n","from sklearn.metrics import accuracy_score\n","import matplotlib.pyplot as plt # to display images from dataset\n","import os\n","from glob import glob\n","from IPython.display import SVG\n","from keras.utils.vis_utils import model_to_dot\n","from keras.utils.vis_utils import plot_model\n","\n","# import tensorflow backend and keras api\n","import tensorflow as tf\n","import keras\n","import keras.backend as K\n","\n","# import model layers and InceptionV3 architecture\n","from tensorflow.python.keras.models import Model\n","from tensorflow.keras.applications.inception_v3 import InceptionV3\n","from sklearn.ensemble import RandomForestClassifier\n","\n","# import optimizers and callbacks\n","from keras.optimizers import SGD\n","from keras.optimizers import Adadelta\n","from keras.optimizers import RMSprop\n","from keras.optimizers import Adam\n","\n","from keras.callbacks import LearningRateScheduler\n","from keras.callbacks import ModelCheckpoint\n","\n","from tensorflow.python.keras.layers import VersionAwareLayers\n","\n","layers = VersionAwareLayers()\n","Dropout = layers.Dropout\n","Dense = layers.Dense\n","Input = layers.Input\n","concatenate = layers.concatenate\n","GlobalAveragePooling2D = layers.GlobalAveragePooling2D\n","AveragePooling2D = layers.AveragePooling2D\n","Flatten = layers.Flatten"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IBGQWkgys07d"},"source":["## Check if CUDA is available"]},{"cell_type":"markdown","metadata":{"id":"GGG4k5GQGq3K"},"source":["CUDA is a parallel computing platform which can speed up our computations (i.e. train this model much faster!) and can be used on any Nvidia GPU from the G8x series onwards.\n","\n","If CUDA is unavailable, check that your runtime has been changed to GPU. (Runtime -> Change runtime type -> GPU)"]},{"cell_type":"code","metadata":{"id":"z1AsnCiTXPzQ"},"source":["print(\"GPU:\", tf.config.list_physical_devices('GPU'), \"\\nCUDA Enabled:\", tf.test.is_built_with_cuda(), \"\\nGPU Name:\", tf.test.gpu_device_name(), \"\\nVisible Devices:\", tf.config.experimental.list_physical_devices('GPU'))\n","from tensorflow.python.client import device_lib\n","print(device_lib.list_local_devices())\n","os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5aMtpZrAtABl"},"source":["## Import dataset"]},{"cell_type":"markdown","metadata":{"id":"qcZi50oc-FRx"},"source":["This notebook assumes that you have preprocessed your videos into multiple frames (approximately 20) and formatted them into .jpg files.\n","\n","However, if your data is not processed - don't worry! There are helper methods defined under `Helper Methods` below, such as `Frame Extraction` and `Face Extraction`, to help you use video files for training!\n"]},{"cell_type":"markdown","metadata":{"id":"95V38voOGh0V"},"source":["The required zip file `dataset.zip` is structured as follows\n","```\n","dataset.zip/\n","  dataset/\n","    train/\n","      fake_image/\n","      .  00001/\n","      .  .  frame00001.jpg\n","      .  .  frame10001.jpg\n","      .  .  frame20001.jpg\n","      .  .  ...\n","      .  00002/\n","      .  .  frame00001.jpg\n","      .  .  frame10001.jpg\n","      .  .  frame20001.jpg\n","      .  .  ...\n","      .  00003/\n","      .  .  frame00001.jpg\n","      .  .  frame10001.jpg\n","      .  .  frame20001.jpg\n","      .  .  ...\n","      .  ...\n","      real_image/\n","      image_labels.csv\n","    val/\n","      fake_image/\n","      real_image/\n","      image_labels.csv\n","```"]},{"cell_type":"markdown","metadata":{"id":"eAJK0UibGkVR"},"source":["`image_labels.csv` is structured as follows\n","```\n","filename                        | class\n","fake_image/00000/frame00001.jpg | fake\n","fake_image/00000/frame10001.jpg | fake\n","fake_image/00000/frame20001.jpg | fake\n","...\n","```"]},{"cell_type":"code","metadata":{"id":"CrHciBgKB-IW"},"source":["from google.colab import drive\n","drive.mount('/content/drive/')\n","base_folder = '/content/drive/MyDrive' # Change this path to your desired directory\n","dataset_path = os.path.join(base_folder, \"dataset.zip\") # zip file for training (training + validation) dataset\n","!unzip $dataset_path"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"j7tZfdbRXRcb"},"source":["# ImageDataGenerator loads images into memory in batches of specified size (in this case 16 images per batch)\n","# this avoids possible memory issues\n","train_folder = '/content/dataset/train'\n","val_folder = '/content/dataset/val'\n","df_train = pd.read_csv(train_folder + '/image_labels.csv')\n","df_val = pd.read_csv(val_folder + '/image_labels.csv')\n","\n","datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255) # rescaling normalizes pixel values from the range [0,255] to [0,1]\n","train_set = datagen.flow_from_dataframe(dataframe=df_train, directory=train_folder, classes= ['real', 'fake'], class_mode=\"categorical\", target_size=(255, 255), batch_size=16)\n","val_set = datagen.flow_from_dataframe(dataframe=df_val, directory=val_folder, classes= ['real', 'fake'], class_mode=\"categorical\", target_size=(255, 255), batch_size=16)\n","\n","print(\"Check class name mapping to label index:\")\n","print(train_set.class_indices)\n","print(val_set.class_indices)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Dy44mQgptHMt"},"source":["## Model architecture"]},{"cell_type":"markdown","metadata":{"id":"AXnVTccS18BF"},"source":["### Load InceptionV3 Pretrained Model"]},{"cell_type":"markdown","metadata":{"id":"tjYgNefKJUPu"},"source":["> InceptionV3 documentation: https://keras.io/api/applications/inceptionv3/"]},{"cell_type":"code","metadata":{"id":"rDJ6yjS5oIEz"},"source":["base_model = InceptionV3(include_top=False, weights=\"imagenet\", input_tensor=Input(shape=(255,255,3)))\n","# add global pooling and dense layers to obtain output from the model\n","layer = base_model\n","layer = GlobalAveragePooling2D()(layer.output)\n","layer = Dense(2, activation='softmax', name='output')(layer)\n","input_layer = base_model.input\n","\n","model = Model(inputs=input_layer, outputs=layer, name=\"InceptionV3\")\n","# display model summary\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ak9xGPE92W1k"},"source":["<a name=\"Optimisers\"></a>\n","### Optimiser"]},{"cell_type":"markdown","metadata":{"id":"4dP331yRJykh"},"source":["> Updates model in response to output of loss function (loss function tells the model the error in classification of individual samples)"]},{"cell_type":"code","metadata":{"id":"3lK88CBJ6w7n"},"source":["# optimisers from Keras https://keras.io/api/optimizers/#available-optimizers\n","sgd = SGD(learning_rate=0.003, momentum=0.9, nesterov=False)\n","adadelta = Adadelta(learning_rate=0.001, rho=0.95, epsilon=1e-07)\n","rmsprop = RMSprop(learning_rate=0.001, rho=0.9, momentum=0.9, epsilon=1e-07, centered=False)\n","\n","lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n","    initial_learning_rate=1e-3,\n","    decay_steps=16,\n","    decay_rate=0.99)\n","\n","adam = Adam(\n","    learning_rate=lr_schedule,\n","    beta_1=0.9,\n","    beta_2=0.999,\n","    epsilon=1e-07,\n","    amsgrad=False,\n","    name=\"Adam\"\n",")\n","\n","#compile model\n","model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JrvKOUJiJjB8"},"source":["### Callback"]},{"cell_type":"code","metadata":{"id":"EjwvgvbcDz-k"},"source":["!mkdir \"/content/weights/\"\n","# ModelCheckpoint callback saves the model weights after every epoch (iteration through the dataset)\n","# if the validation accuracy is higher than that of the model previously saved\n","checkpoint = ModelCheckpoint(\"/content/weights/inceptionv3.h5\", monitor='val_loss', verbose=1, save_best_only=True, mode='auto')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BOevAqZ_uvku"},"source":["## Train model"]},{"cell_type":"code","metadata":{"id":"K6MK2m5pXC6n"},"source":["hist = model.fit(train_set, batch_size = 16, steps_per_epoch = 16, epochs = 500, validation_data=val_set, validation_steps = 64, callbacks = [checkpoint])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CbRd5xQvuzbq"},"source":["## Display model training/validation accuracy and loss"]},{"cell_type":"code","metadata":{"id":"UiKZSiIWDX6Z"},"source":["# plot training and validation accuracy against epochs using matplotlib\n","plt.plot(hist.history['accuracy'])\n","plt.plot(hist.history['val_accuracy'])\n","plt.title('model accuracy')\n","plt.ylabel('accuracy')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'validation'], loc='upper left')\n","plt.show()\n","\n","# plot training and validation loss against epochs using matplotlib\n","plt.plot(hist.history['loss'])\n","plt.plot(hist.history['val_loss'])\n","plt.title('model loss')\n","plt.ylabel('loss')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'validation'], loc='upper left')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IrWRmWtu8Ylc"},"source":["## Load Weights of the best model trained so far"]},{"cell_type":"code","metadata":{"id":"qebLMWrL8nE6"},"source":["model.load_weights(\"/content/weights/inceptionv3.h5\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JfNia68TBR7Y"},"source":["<a name=\"eval\"></a>\n","## Evaluate Model"]},{"cell_type":"markdown","metadata":{"id":"aewSe6eQKFJ6"},"source":["To classify the video as real or fake, we will first obtain the model's predictions on each frame extracted from the video. We then take the mean of the prediction (probability) of the frames as the prediction of the video. "]},{"cell_type":"code","metadata":{"id":"-Lr1LS26fNk4"},"source":["def read_image_from_disk(path):\n","\n","  \"\"\"\n","  Helper function to read image from disk given a absolute path.\n","\n","  :param path: Absolute path to image file on disk\n","  :return: Image in Numpy Ndarray representation\n","  \"\"\"\n","\n","  img = tf.keras.preprocessing.image.load_img(path, target_size=(255,255,3))\n","  img = tf.keras.preprocessing.image.img_to_array(img)\n","  img = img/255\n","  img = np.expand_dims(img, axis=0)\n","\n","  return img\n","\n","\n","def get_frames_to_vid_mapping(frame_list):\n","\n","  \"\"\"\n","  Helper function to generate a mapping of frames to it's corresponding video \n","  name.\n","\n","  The path of frames in the frame_list will be in such format:\n","  image/[video name]/[frame number].jpg\n","  e.g. image/00000/00032.jpg\n","\n","  :param frame_list: A list of paths to the image frames\n","  :return: A sorted dictionary with keys as the video name and value as the\n","           corresponding frames.\n","           e.g. of returned mapping dictionary:\n","\n","            {\n","              \"00000\":[\n","                  \"00032\",\n","                  \"00064\",\n","                  .\n","                  .\n","                  .\n","                  \"00487\"\n","              ],\n","              \"00001\":[\n","                  \"00000\",\n","                  \"00032\",\n","                  .\n","                  .\n","                  .\n","                  \"00392\"\n","              ],\n","              .\n","              .\n","              .\n","              \"00790\":[\n","                  \"00000\",\n","                  \"00027\",\n","                  .\n","                  .\n","                  .\n","                  \"00542\"\n","              ]\n","            }\n","  \"\"\"\n","\n","  # Get all videos name\n","  vidnames = [frame.split(\"/\")[1:2][0] for frame in frame_list]\n","  # Get only unique names\n","  vidnames = set(vidnames)\n","  # Init the mapping dict\n","  mapping = {vidname: [] for vidname in vidnames}\n","\n","  # Add frames to to its corresponding list\n","  for frame in frame_list:\n","    vidname = str(frame.split(\"/\")[1:2][0])\n","    frame_number = str(frame.split(\"/\")[-1].split(\".\")[0])\n","    mapping[vidname].append(frame_number)\n","\n","  return dict(sorted(mapping.items()))\n","\n","\n","def infer_videos(test_data_path, csv_file, num_of_videos='All'):\n","\n","  \"\"\"\n","  Function to infer a test data set. The function takes in a path to the test\n","  data set and a csv file that contain the paths of the frames extracted from \n","  the videos in the test dataset.\n","\n","  :param test_data_path: Absoulute path to the test dataset\n","  :param csv_file: File Name of the CSV file that must be in the test_data_path\n","  :param num_of_videos: Number of videos to infer from the dataset (default: All)\n","  :return: Pandas dataframe which contains the prediction (probability of being \n","           fake) of each video. \n","  \"\"\"\n","\n","  list_dir = list(pd.read_csv(test_data_path + csv_file).iloc[:,0])\n","\n","  mapping = get_frames_to_vid_mapping(list_dir)\n","\n","  # [*mapping] gives the list of keys (video name) in the mapping dict\n","  num_of_videos_avail = len([*mapping])\n","\n","  # Set number of videos to be inferred to total of videos available if given \n","  # num_of_videos is more than max amount of available videos\n","  if num_of_videos == 'All' or num_of_videos > num_of_videos_avail:\n","      num_of_videos = num_of_videos_avail\n","\n","  # init mapping of videos to its corresponding predicted probabilities\n","  videos_to_prediction = {}\n","\n","  # Loop through each video and make a prediction of each frame in the video.\n","  # Assigned a prediction to each video by taking the mean of its corresponding\n","  # frames' probabilities.\n","  for video_name in [*mapping][0:num_of_videos]:\n","\n","    frames = mapping[video_name]\n","    predictions = []\n","    print(\"Infering video {video}...\".format(video=video_name))\n","    print(\"Processing frame \", end=\" \")\n","\n","    # Process each frame in video\n","    for frame in frames:\n","      print(frame, end =\", \")\n","      frame_path = \"image/{video_name}/{frame}.jpg\".format(video_name=video_name, frame=frame)\n","      img = read_image_from_disk(test_data_path + frame_path)\n","      prediction = model.predict(img)[0]\n","      # Collect only the 'real' side of probability\n","      predictions.append(prediction[1])\n","\n","    # Take the mean of the probabilities from the frames\n","    videos_to_prediction[video_name] = statistics.mean(predictions)\n","    print(\"Done!\")\n","  \n","  return pd.DataFrame(videos_to_prediction.items())\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aXR2qGUMDEU_"},"source":["modelPredictions = infer_videos(\"/content/ffdata/test/\", \"image_labels.csv\")\n","print(modelPredictions)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z4ksR0eGu42g"},"source":["## Generate Predictions"]},{"cell_type":"code","metadata":{"id":"mwEx87-B6WJR"},"source":["dataset_path = os.path.join(base_folder, \"model_predictions.csv\") # zip file for predictions\n","modelPredictions.columns = ['vid_name', 'label']\n","modelPredictions.to_csv(dataset_path, index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"92qQiIYPZMms"},"source":["## Helper Methods"]},{"cell_type":"markdown","metadata":{"id":"cgT-OF0xLYz8"},"source":["### Frame Extraction"]},{"cell_type":"code","metadata":{"id":"-4m1ZfIsZPn0"},"source":["def extract_frames(vidPath, savePath):\n","  for file in os.listdir(vidPath):\n","    count = 0\n","    cap = cv2.VideoCapture(vidPath + file)\n","    frame_rate = 3\n","    prev = 0\n","    i = 0\n","    while cap.isOpened():\n","      time_elapsed = time.time() - prev\n","      ret, frame = cap.read()\n","      if not ret:\n","        break\n","      if time_elapsed > 1./frame_rate:\n","          prev = time.time()\n","          os.chdir(savePath)\n","          filename = file + \"-frame%d.jpg\" % count;count+=1\n","          cv2.imwrite(filename, frame)\n","          i += 1\n","    cap.release()\n","    cv2.destroyAllWindows()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fMujw8M2ZQmV"},"source":["### Face Extraction"]},{"cell_type":"code","metadata":{"id":"M_U_zeQ4ZzFs"},"source":["def extF(imgPath, savePath):\n","    for file in os.listdir(imgPath):\n","        face_detector = dlib.get_frontal_face_detector()\n","        image = io.imread(imgPath + file)\n","        detected_faces = face_detector(image, 1)\n","        face_frames = [(x.left(), x.top(), x.right(), x.bottom()) for x in detected_faces]\n","        count = 0\n","        for n, face_rect in enumerate(face_frames):\n","            os.chdir(savePath)\n","            face = Image.fromarray(image).crop(face_rect)\n","            image = np.asanyarray(face)\n","            filename = file + \"-\" + str(count) + \".jpg\"\n","            io.imsave(filename, image)\n","            count += 1\n","            break\n"],"execution_count":null,"outputs":[]}]}